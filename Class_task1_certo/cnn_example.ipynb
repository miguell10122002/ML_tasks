{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import sklearn\n",
    "from sklearn.model_selection import train_test_split\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from keras import layers, models\n",
    "from keras.layers import RandomBrightness, RandomZoom, RandomTranslation, RandomContrast\n",
    "from keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "from keras.utils import to_categorical\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "import keras.backend as K\n",
    "\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "np.random.seed = 515005\n",
    "\n",
    "x = np.load('Xtrain_Classification1.npy')\n",
    "X_t = np.load('Xtest_Classification1.npy')\n",
    "y = np.load('ytrain_Classification1.npy')\n",
    "x = np.reshape(x, (6254, 28, 28, 3))\n",
    "X_t = np.reshape(X_t, (1764, 28, 28, 3))\n",
    "x = x.astype('float32')/ 255.0\n",
    "y = to_categorical(y,num_classes=2)\n",
    "x_train, x_temp, y_train, y_temp = train_test_split(x, y, train_size=0.8, random_state=42)\n",
    "\n",
    "class FalsePositives(tf.keras.metrics.Metric):\n",
    "    def __init__(self, name='false_positives', **kwargs):\n",
    "        super(FalsePositives, self).__init__(name=name, **kwargs)\n",
    "        self.false_positives = self.add_weight(name='fp', initializer='zeros')\n",
    "\n",
    "    def update_state(self, y_true, y_pred, sample_weight=None):\n",
    "        y_true_binary = K.argmax(y_true, axis=-1)\n",
    "        y_pred_binary = K.argmax(y_pred, axis=-1)\n",
    "        false_positives = K.sum(K.cast(K.equal(y_true_binary, 0) & K.equal(y_pred_binary, 1), 'float'))\n",
    "        self.false_positives.assign_add(false_positives)\n",
    "\n",
    "    def result(self):\n",
    "        return self.false_positives\n",
    "\n",
    "class FalseNegatives(tf.keras.metrics.Metric):\n",
    "    def __init__(self, name='false_negatives', **kwargs):\n",
    "        super(FalseNegatives, self).__init__(name=name, **kwargs)\n",
    "        self.false_negatives = self.add_weight(name='fn', initializer='zeros')\n",
    "\n",
    "    def update_state(self, y_true, y_pred, sample_weight=None):\n",
    "        y_true_binary = K.argmax(y_true, axis=-1)\n",
    "        y_pred_binary = K.argmax(y_pred, axis=-1)\n",
    "        false_negatives = K.sum(K.cast(K.equal(y_true_binary, 1) & K.equal(y_pred_binary, 0), 'float'))\n",
    "        self.false_negatives.assign_add(false_negatives)\n",
    "\n",
    "    def result(self):\n",
    "        return self.false_negatives\n",
    "class TrueNegatives(tf.keras.metrics.Metric):\n",
    "    def __init__(self, name='true_negatives', **kwargs):\n",
    "        super(TrueNegatives, self).__init__(name=name, **kwargs)\n",
    "        self.true_negatives = self.add_weight(name='tn', initializer='zeros')\n",
    "\n",
    "    def update_state(self, y_true, y_pred, sample_weight=None):\n",
    "        y_true_binary = K.argmax(y_true, axis=-1)\n",
    "        y_pred_binary = K.argmax(y_pred, axis=-1)\n",
    "        true_negatives = K.sum(K.cast(K.equal(y_true_binary, 0) & K.equal(y_pred_binary, 0), 'float'))\n",
    "        self.true_negatives.assign_add(true_negatives)\n",
    "\n",
    "    def result(self):\n",
    "        return self.true_negatives\n",
    "\n",
    "class TruePositives(tf.keras.metrics.Metric):\n",
    "    def __init__(self, name='true_positives', **kwargs):\n",
    "        super(TruePositives, self).__init__(name=name, **kwargs)\n",
    "        self.true_positives = self.add_weight(name='tp', initializer='zeros')\n",
    "\n",
    "    def update_state(self, y_true, y_pred, sample_weight=None):\n",
    "        y_true_binary = K.argmax(y_true, axis=-1)\n",
    "        y_pred_binary = K.argmax(y_pred, axis=-1)\n",
    "        true_positives = K.sum(K.cast(K.equal(y_true_binary, 1) & K.equal(y_pred_binary, 1), 'float'))\n",
    "        self.true_positives.assign_add(true_positives)\n",
    "\n",
    "    def result(self):\n",
    "        return self.true_positives\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "  \n",
    "\n",
    "\n",
    "\n",
    "class Weighted_BCE_Loss(keras.losses.Loss):\n",
    "    def __init__(self, weight_zero = 0.15, weight_one = 0.85):\n",
    "        super().__init__()\n",
    "        self.weight_zero = weight_zero\n",
    "        self.weight_one = weight_one\n",
    "    def call(self, y_true, y_pred):        \n",
    "        bin_crossentropy = K.binary_crossentropy(y_true, y_pred)\n",
    "    \n",
    "        \n",
    "        weights = y_true * self.weight_one + (1. - y_true) * self.weight_zero\n",
    "        weighted_bin_crossentropy = weights * bin_crossentropy \n",
    "\n",
    "        return keras.backend.mean(weighted_bin_crossentropy)\n",
    "\n",
    "def weighted_bincrossentropy(true, pred, weight_zero = 0.25, weight_one = 1):\n",
    "    \"\"\"\n",
    "    Calculates weighted binary cross entropy. The weights are fixed.\n",
    "        \n",
    "    This can be useful for unbalanced catagories.\n",
    "    \n",
    "    Adjust the weights here depending on what is required.\n",
    "    \n",
    "    For example if there are 10x as many positive classes as negative classes,\n",
    "        if you adjust weight_zero = 1.0, weight_one = 0.1, then false positives \n",
    "        will be penalize 10 times as much as false negatives.\n",
    "\n",
    "    \"\"\"\n",
    "  \n",
    "   \n",
    "    bin_crossentropy = keras.backend.binary_crossentropy(true, pred)\n",
    "    \n",
    "    \n",
    "    weights = true * weight_one + (1. - true) * weight_zero\n",
    "    weighted_bin_crossentropy = weights * bin_crossentropy \n",
    "\n",
    "    return K.mean(weighted_bin_crossentropy)\n",
    "\n",
    "model = keras.Sequential([\n",
    "    layers.Conv2D(32, (3, 3), activation='relu', input_shape=(28, 28, 3)),\n",
    "    layers.BatchNormalization(),\n",
    "    layers.MaxPooling2D(pool_size=(2, 2)),\n",
    "    layers.Dropout(0.05),\n",
    "    layers.Conv2D(64, (3, 3), activation='relu'),\n",
    "    layers.BatchNormalization(),\n",
    "    layers.MaxPooling2D(pool_size=(2, 2)),\n",
    "    layers.Dropout(0.05),\n",
    "    layers.Conv2D(64, (3, 3), activation='relu'),\n",
    "    layers.BatchNormalization(),\n",
    "    layers.MaxPooling2D(pool_size=(2, 2)),\n",
    "    layers.Dropout(0.05),\n",
    "    layers.Flatten(),\n",
    "    layers.Dense(128, activation='relu'),\n",
    "    layers.Dropout(0.15),\n",
    "    layers.Dense(2, activation='softmax')\n",
    "])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    " \n",
    "\n",
    "\n",
    "class TrainBalancedAccuracyCallback(tf.keras.callbacks.Callback):\n",
    "    def __init__(self):\n",
    "        super(TrainBalancedAccuracyCallback, self).__init__()\n",
    "        \n",
    "\n",
    "    def on_epoch_end(self, epoch, logs={}):\n",
    "       \n",
    "\n",
    "        train_sensitivity = logs['tp'] / (logs['tp'] + logs['fn'])\n",
    "        train_specificity = logs['tn'] / (logs['tn'] + logs['fp'])\n",
    "        logs['train_sensitivity'] = train_sensitivity\n",
    "        logs['train_specificity'] = train_specificity\n",
    "        logs['train_balacc'] = (train_sensitivity + train_specificity) / 2\n",
    "        print(' train_balacc', logs['train_balacc'])\n",
    "\n",
    "\n",
    "class ValBalancedAccuracyCallback(tf.keras.callbacks.Callback):\n",
    "    def __init__(self):\n",
    "        super(ValBalancedAccuracyCallback, self).__init__()\n",
    "      \n",
    "\n",
    "    def on_epoch_end(self, epoch, logs={}):\n",
    "        \n",
    "        \n",
    "        val_sensitivity = logs['val_tp'] / (logs['val_tp'] + logs['val_fn'])\n",
    "        val_specificity = logs['val_tn'] / (logs['val_tn'] + logs['val_fp'])\n",
    "        logs['val_sensitivity'] = val_sensitivity\n",
    "        logs['val_specificity'] = val_specificity\n",
    "        logs['val_balacc'] = (val_sensitivity + val_specificity) / 2\n",
    "        print(' val_balacc', logs['val_balacc'])\n",
    "        \n",
    "METRICS = [\n",
    "    TruePositives(name='tp'),\n",
    "    FalsePositives(name='fp'),\n",
    "    TrueNegatives(name='tn'),\n",
    "    FalseNegatives(name='fn'),\n",
    "    keras.metrics.Precision(name='precision'),\n",
    "    keras.metrics.Recall(name='recall'),\n",
    "    keras.metrics.AUC(name='auc'),\n",
    "    keras.metrics.AUC(name='prc', curve='PR'),\n",
    "    \n",
    "    \n",
    "]\n",
    "        \n",
    "\n",
    "        \n",
    "lr_scheduler = ReduceLROnPlateau(factor=0.8, patience=5, min_lr=1e-7, verbose=1)\n",
    "\n",
    "\n",
    "optimizer = keras.optimizers.legacy.Adam(learning_rate=0.001)\n",
    "model.compile(optimizer=optimizer, loss=Weighted_BCE_Loss(weight_zero=0.16, weight_one=0.84), metrics=METRICS)\n",
    "datagen = ImageDataGenerator(\n",
    "    rotation_range=40,\n",
    "    width_shift_range=0.2,\n",
    "    height_shift_range=0.2,\n",
    "    shear_range=0.2,\n",
    "    zoom_range=0.2,\n",
    "    horizontal_flip=True,\n",
    "    fill_mode='nearest'\n",
    ")\n",
    "datagen.fit(x_train)\n",
    "datagen.fit(x_temp)\n",
    "train_generator = datagen.flow(x_train, y_train, batch_size=32)\n",
    "val_generator = datagen.flow(x_temp,y_temp, batch_size=32)\n",
    "\n",
    "train_balanced_accuracy_callback = TrainBalancedAccuracyCallback()\n",
    "val_balanced_accuracy_callback = ValBalancedAccuracyCallback()\n",
    "\n",
    "model.fit(train_generator, epochs=100, validation_data=val_generator, batch_size= 32,\n",
    "          callbacks=[lr_scheduler, train_balanced_accuracy_callback, val_balanced_accuracy_callback], verbose = 0)\n",
    "\n",
    "\n",
    "X_t = X_t.astype('float32') / 255.0\n",
    "y_t = model.predict(X_t)\n",
    "y_t = (y_t[:, 1] > 0.5).astype(int)   \n",
    "y_t = np.reshape(y_t, (-1, 1))\n",
    "np.save('ytest_Classification1.npy', y_t)\n",
    "print(y_t)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import numpy as np\n",
    "y_x = np.load('ytest_Classification1.npy')\n",
    "print(y_x)\n",
    "w = len(y_x)\n",
    "print(w)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
